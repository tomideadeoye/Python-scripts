{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#web scraping thingy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# return all div elements with a class attribute containing the text \\'listing-col-\\':\\nfor EachPart in soup.select(\\'div[class*=\"listing-col-\"]\\'):\\n    print EachPart.get_text()\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\"type\", \"beds\", \"price\", \"location\", \"url\", \"aquisition\", \"contact\", \"duration\", \"furnished\", \"serviced\", \"imageurl\"] //compare rent vs buy(month v day), should I buy/recommeded\n",
    "# paginationSelector = soup.find('div', class_='pagination-area').nav.ul.findAll('li')[3].a['href']\n",
    "# paginationSelector\n",
    "\n",
    "'''\n",
    "# return all div elements with a class attribute containing the text 'listing-col-':\n",
    "for EachPart in soup.select('div[class*=\"listing-col-\"]'):\n",
    "    print EachPart.get_text()\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\"\n",
    "\n",
    "https://www.smartcontractresearch.org/latest\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yt/r5vqk9zd7f35c460q8mztp2w0000gn/T/ipykernel_7316/2375564607.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('./chromedriver')\n"
     ]
    }
   ],
   "source": [
    "def getPageData(url):\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    driver.get(url)\n",
    "    time.sleep(10)     \n",
    "    SCROLL_PAUSE_TIME = 10\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "    # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    result = []\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')   \n",
    "    scrf_summaries =  soup.find_all(\"tr\")\n",
    "    for post in scrf_summaries:\n",
    "        if post.find('a', href=True):\n",
    "            post_url = post.find('a', href=True)['href']\n",
    "            if post_url.startswith('/'):\n",
    "                post_url = 'https://www.smartcontractresearch.org' + post_url\n",
    "            result.append(post_url)\n",
    "\n",
    "    driver.close()\n",
    "    return result\n",
    "\n",
    "    \n",
    "result =  getPageData(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##UTILITIES\n",
    "\n",
    "# WRITE DATA TO CSV FILE\n",
    "# data is a list of lists\n",
    "\n",
    "def createCsvFile(filename, rose):\n",
    "    header =[ \"title\", \"url\", \"category\", \"total_likes\", \"paper link\", \"tags\", \"comments\"]\n",
    "    with open(filename, 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerow(header)\n",
    "        write.writerows(rose)\n",
    "\n",
    "def unique(list1):\n",
    "    unique_list = []\n",
    "    for x in list1:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = unique(result)\n",
    "len(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalPosts = []\n",
    "header =[ \"title\", \"url\", \"category\", \"total_likes\", \"paper link\", \"tags\", \"comments\"]\n",
    "\n",
    "def getPostComments(url):\n",
    "    driver = webdriver.Chrome('./chromedriver') \n",
    "    result = []\n",
    "    driver.get(url)\n",
    "    time.sleep(10) \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser') \n",
    "    # classes = soup.find_all(class_=True)\n",
    "    category = soup.find(class_='category-name').text or ''\n",
    "    tags = soup.find_all(\"a\", class_='discourse-tag') or [\"\"]\n",
    "    tags = [tag.text for tag in tags]  \n",
    "    tags = unique(tags)\n",
    "    title= soup.find(class_='fancy-title').text.replace('\\n', '')  or ''\n",
    "    comments = soup.find_all(\"div\", class_='cooked') or ['']\n",
    "    comments = [comment.text for comment in comments]\n",
    "    comments = [comment.replace('\\n', '') for comment in comments] \n",
    "    total_likes = soup.find_all(class_='number')[3].text or \"\"\n",
    "    links = soup.find_all('a', href=True) or [\"\"]\n",
    "    returnedLink = []\n",
    "    for link in links:\n",
    "        if \"citation\" in link['href']:\n",
    "            index = links.index(link)\n",
    "            returnedLink.append(links[index + 1][\"href\"]) \n",
    "\n",
    "    if len(comments) - 1 >= 10:\n",
    "        finalPosts.append([ title, url, category, total_likes, unique(returnedLink), tags, comments])\n",
    "        \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in result:\n",
    "    # getPostComments(post[0])\n",
    "    try:\n",
    "        getPostComments(post)\n",
    "        print(result.index(post), \" of \", len(result))\n",
    "        print(finalPosts[-1][0:2])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"error on \" + post)\n",
    "        print(e)\n",
    "\n",
    "\n",
    "createCsvFile('scrfabove10.csv', finalPosts)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE DATA TO CSV FILE\n",
    "resultHeader = [\"url\", \"title\"]\n",
    "createCsvFile('scrfabove10.csv', resultHeader, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "propertyForSale = pd.read_csv(\"ScrapedSalePropertyDataSet.csv\")\n",
    "propertyForRent = pd.read_csv(\"ScrapedRentalDataSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def abstractedReadToDataFrame(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = abstractedReadToDataFrame(\"scrfabove10.csv\")\n",
    "df.sample(20)\n",
    "df.sort_values(by=[\"total_likes\"], ascending=False) \n",
    "\n",
    "# where title starts with \"A\"\n",
    "# df[df['title'].str.startswith(\"R\")].sort_values(by=[\"total_likes\"], ascending=False)\n",
    "df[df['title'].str.contains(\"r\")]\n",
    "# print title for all rows\n",
    "df['title'].values\n",
    "\n",
    "# strip whitespace from title\n",
    "df['title'] = df['title'].str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>total_likes</th>\n",
       "      <th>paper link</th>\n",
       "      <th>tags</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, url, category, total_likes, paper link, tags, comments]\n",
       "Index: []"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['title'].values\n",
    "# df['title'].str.contains(\"research\")\n",
    "# convert title to string\n",
    "# df['title'] = df['title'].astype(str)\n",
    "from tabnanny import check\n",
    "\n",
    "\n",
    "summaries_only = df[df['url'].str.contains(\"summary\")]  \n",
    "\n",
    "sorted_summaries = summaries_only.sort_values(by=[\"total_likes\"], ascending=False)\n",
    "sorted_summaries\n",
    "whereTagsShort = sorted_summaries[sorted_summaries['tags'].str.len() == 2]\n",
    "whereTagsShort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "try:\n",
    "    sorted_summaries['tags'] = sorted_summaries['tags'].apply(literal_eval)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalability', 'iot', 'summary']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_summaries['tags'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leveraging Blockchain for Greater Accessibility of Machine Learning Models\n",
      "SoK: Applying Blockchain Technology in Industrial Internet of Things\n",
      "On the Economic Design of Stablecoins\n",
      "Ethereum Name Service: the Good, the Bad, and the Ugly\n",
      "Research Summary - What is in Your Wallet? Privacy and Security Issues in Web 3.0\n",
      "Using Distributed Ledger Technologies to Improve and Maximise the Collection of Property Taxes\n",
      "Towards A first step to understand flash loan and its application in Defi Ecosystem\n"
     ]
    }
   ],
   "source": [
    "whereTagsShort = sorted_summaries[sorted_summaries['tags'].str.len() < 3 ]\n",
    "whereTagsShort = whereTagsShort[whereTagsShort[\"comments\"].str.len() > 10]\n",
    "whereTagsShort[\"title\"]\n",
    "for i in whereTagsShort[\"title\"]:\n",
    "    print(i.replace('Research Summary:', '').strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leveraging Blockchain for Greater Accessibility of Machine Learning Models https://www.smartcontractresearch.org/t/research-summary-leveraging-blockchain-for-greater-accessibility-of-machine-learning-models/808\n",
      "SoK: Applying Blockchain Technology in Industrial Internet of Things https://www.smartcontractresearch.org/t/research-summary-sok-applying-blockchain-technology-in-industrial-internet-of-things/866\n",
      "On the Economic Design of Stablecoins https://www.smartcontractresearch.org/t/research-summary-on-the-economic-design-of-stablecoins/817\n",
      "Ethereum Name Service: the Good, the Bad, and the Ugly https://www.smartcontractresearch.org/t/research-summary-ethereum-name-service-the-good-the-bad-and-the-ugly/1587\n",
      "Research Summary - What is in Your Wallet? Privacy and Security Issues in Web 3.0 https://www.smartcontractresearch.org/t/research-summary-what-is-in-your-wallet-privacy-and-security-issues-in-web-3-0/1225\n",
      "Using Distributed Ledger Technologies to Improve and Maximise the Collection of Property Taxes https://www.smartcontractresearch.org/t/research-summary-using-distributed-ledger-technologies-to-improve-and-maximise-the-collection-of-property-taxes/1176\n",
      "Towards A first step to understand flash loan and its application in Defi Ecosystem https://www.smartcontractresearch.org/t/research-summary-towards-a-first-step-to-understand-flash-loan-and-its-application-in-defi-ecosystem/1551\n"
     ]
    }
   ],
   "source": [
    "# print title and url for all rows\n",
    "for i in range(len(whereTagsShort)):\n",
    "    print(whereTagsShort[\"title\"].iloc[i].replace('Research Summary:', '').strip(), whereTagsShort[\"url\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
